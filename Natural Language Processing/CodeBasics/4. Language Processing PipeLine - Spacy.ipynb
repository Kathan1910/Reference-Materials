{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e23153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707c1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad80832",
   "metadata": {},
   "source": [
    "# Blank NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79720e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "993bad96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb0150",
   "metadata": {},
   "source": [
    "### Download Trained Pipeline\n",
    "To download trained pipeline use a command such as,\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "This downloads the small (sm) pipeline for english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2136a08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "652603eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fa9f5c208e0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fa9f5c20040>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fa9f5da1e40>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7fa9f5a5ec40>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fa9f5a1c080>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fa9f5da1dd0>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776b403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  proper noun  |  Captain\n",
      "america  |  proper noun  |  america\n",
      "ate  |  verb  |  eat\n",
      "100  |  numeral  |  100\n",
      "$  |  numeral  |  $\n",
      "of  |  adposition  |  of\n",
      "samosa  |  proper noun  |  samosa\n",
      ".  |  punctuation  |  .\n",
      "Then  |  adverb  |  then\n",
      "he  |  pronoun  |  he\n",
      "said  |  verb  |  say\n",
      "I  |  pronoun  |  I\n",
      "can  |  auxiliary  |  can\n",
      "do  |  verb  |  do\n",
      "this  |  pronoun  |  this\n",
      "all  |  determiner  |  all\n",
      "day  |  noun  |  day\n",
      ".  |  punctuation  |  .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b170e994",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0ef284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4b4d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d041311",
   "metadata": {},
   "source": [
    "### Trained processing pipeline in French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d0ca5",
   "metadata": {},
   "source": [
    "- You need to install the processing pipeline for french language using this command,\n",
    "\n",
    "- python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8688d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:04:19.598278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Collecting fr-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.5.0/fr_core_news_sm-3.5.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from fr-core-news-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (60.10.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/kathan/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kathan/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kathan/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kathan/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kathan/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kathan/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/kathan/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/kathan/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/kathan/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kathan/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.1)\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef35f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6899aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3013c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  PROPN  |  Tesla\n",
      "Inc  |  PROPN  |  Inc\n",
      "va  |  VERB  |  aller\n",
      "racheter  |  VERB  |  racheter\n",
      "Twitter  |  VERB  |  twitter\n",
      "pour  |  ADP  |  pour\n",
      "$  |  NOUN  |  dollar\n",
      "45  |  NUM  |  45\n",
      "milliards  |  NOUN  |  milliard\n",
      "de  |  ADP  |  de\n",
      "dollars  |  NOUN  |  dollar\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ad2ca",
   "metadata": {},
   "source": [
    "# Adding a component to a blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af152f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09668cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84f02295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b8a59",
   "metadata": {},
   "source": [
    "# Excersie: 1\n",
    "- Get all the proper nouns from a given text in a list and also count how many of them.\n",
    "- Proper Noun means a noun that names a particular person, place, or thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d34f331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proper Nouns:  []\n",
      "Count:  0\n"
     ]
    }
   ],
   "source": [
    "text = '''Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "'''\n",
    "\n",
    "# https://spacy.io/usage/linguistic-features\n",
    "\n",
    "#creating the nlp object\n",
    "doc = nlp(text)   \n",
    "\n",
    "\n",
    "#list for storing the proper nouns\n",
    "all_proper_nouns = []  \n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":        #checking the whether token belongs to parts of speech \"PROPN\" [Proper Noun]\n",
    "        all_proper_nouns.append(token)\n",
    "  \n",
    "\n",
    " #finally printing the results\n",
    "print(\"Proper Nouns: \", all_proper_nouns)\n",
    "print(\"Count: \", len(all_proper_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933fb58",
   "metadata": {},
   "source": [
    "# Excersie: 2\n",
    "- Get all companies names from a given text and also the count of them.\n",
    "- Hint: Use the spacy ner functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "634fc462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Names:  [Tesla, Walmart, Amazon, Microsoft, Google, Infosys, Reliance, HDFC Bank, Hindustan Unilever, Bharti]\n",
      "Count:  10\n"
     ]
    }
   ],
   "source": [
    "text = '''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#list for storing the company names\n",
    "all_company_names = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'ORG':     #checking the whether token belongs to entity \"ORG\" [Organisation]\n",
    "        all_company_names.append(ent)\n",
    "\n",
    "\n",
    "\n",
    "#finally printing the results\n",
    "print(\"Company Names: \", all_company_names)\n",
    "print(\"Count: \", len(all_company_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4faf1",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
